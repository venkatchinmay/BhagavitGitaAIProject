{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bhagavad Gita AI Assistant üïâÔ∏è\n",
    "\n",
    "This notebook implements a RAG (Retrieval-Augmented Generation) system that:\n",
    "1. **Loads** all 700+ verses from the Bhagavad Gita API\n",
    "2. **Stores** them in a vector database (ChromaDB) with semantic embeddings\n",
    "3. **Retrieves** relevant slokas based on user problems\n",
    "4. **Generates** wisdom-based solutions using an LLM\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "User Problem ‚Üí Semantic Search ‚Üí Top-K Slokas ‚Üí LLM Reasoning ‚Üí Solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "\n",
    "Install dependencies first:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ollama\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DB_PATH = \"./bg_vector_db\"\n",
    "COLLECTION_NAME = \"bhagavad_gita\"\n",
    "EMBEDDING_MODEL = 'paraphrase-multilingual-MiniLM-L12-v2'  # Supports English and Sanskrit\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 2  # seconds\n",
    "\n",
    "# Chapter structure (number of verses per chapter)\n",
    "# Source: https://github.com/vedicscriptures/bhagavad-gita\n",
    "CHAPTER_VERSES = {\n",
    "    1: 47, 2: 72, 3: 43, 4: 42, 5: 29, 6: 47,\n",
    "    7: 30, 8: 28, 9: 34, 10: 42, 11: 55, 12: 20,\n",
    "    13: 35, 14: 27, 15: 20, 16: 24, 17: 28, 18: 78\n",
    "}\n",
    "\n",
    "# Ollama Configuration (running in Kubernetes)\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://ollama.ollama.svc.cluster.local:11434\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.2\")\n",
    "print(f\"üìÅ Database path: {DB_PATH}\")\n",
    "print(f\"ü§ñ Embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"ü¶ô Ollama URL: {OLLAMA_BASE_URL}\")\n",
    "print(f\"üß† Ollama model: {OLLAMA_MODEL}\")\n",
    "print(f\"üìö Total verses to load: {sum(CHAPTER_VERSES.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading Functions\n",
    "\n",
    "Enhanced version with:\n",
    "- Error handling and retries\n",
    "- Progress tracking\n",
    "- Rich metadata extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_verse(chapter: int, verse: int, max_retries: int = MAX_RETRIES) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch a single verse from the Bhagavad Gita API with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        chapter: Chapter number (1-18)\n",
    "        verse: Verse number\n",
    "        max_retries: Maximum number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing verse data or None if failed\n",
    "    \"\"\"\n",
    "    url = f\"https://vedicscriptures.github.io/slok/{chapter}/{verse}/\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            elif response.status_code == 404:\n",
    "                # Verse doesn't exist, don't retry\n",
    "                return None\n",
    "            else:\n",
    "                # Other error, retry\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                continue\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                print(f\"\\n‚ùå Failed to fetch {chapter}.{verse} after {max_retries} attempts: {e}\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_verse_data(data: Dict, chapter: int, verse: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract and structure relevant data from API response.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with structured verse data\n",
    "    \"\"\"\n",
    "    # Build comprehensive search text combining all translations and commentaries\n",
    "    search_components = []\n",
    "    \n",
    "    # Add English translation (Sivananda)\n",
    "    siva_translation = data.get('siva', {}).get('et', '')\n",
    "    if siva_translation:\n",
    "        search_components.append(siva_translation)\n",
    "    \n",
    "    # Add Sivananda commentary\n",
    "    siva_commentary = data.get('siva', {}).get('ec', '')\n",
    "    if siva_commentary:\n",
    "        search_components.append(siva_commentary)\n",
    "    \n",
    "    # Add Chinmayananda commentary (often very insightful)\n",
    "    chinmay_commentary = data.get('chinmay', {}).get('ec', '')\n",
    "    if chinmay_commentary:\n",
    "        search_components.append(chinmay_commentary)\n",
    "    \n",
    "    # Add Purohit Swami translation (simpler English)\n",
    "    purohit_translation = data.get('purohit', {}).get('et', '')\n",
    "    if purohit_translation:\n",
    "        search_components.append(purohit_translation)\n",
    "    \n",
    "    # Combine all components\n",
    "    search_text = \" \".join(search_components)\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = {\n",
    "        \"chapter\": chapter,\n",
    "        \"verse\": verse,\n",
    "        \"verse_id\": f\"{chapter}.{verse}\",\n",
    "        \"sanskrit\": data.get(\"slok\", \"\"),\n",
    "        \"transliteration\": data.get(\"transliteration\", \"\"),\n",
    "        \"translation\": siva_translation,\n",
    "        \"commentary\": siva_commentary[:500] if siva_commentary else \"\",  # Truncate for metadata\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"id\": f\"{chapter}.{verse}\",\n",
    "        \"search_text\": search_text,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Vector Database\n",
    "\n",
    "Setup ChromaDB with persistent storage and embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "print(\"üîÑ Initializing ChromaDB...\")\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "# Delete existing collection if you want to reload data\n",
    "# Uncomment the following lines to reset the database\n",
    "# try:\n",
    "#     client.delete_collection(name=COLLECTION_NAME)\n",
    "#     print(\"üóëÔ∏è Deleted existing collection\")\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# Get or create collection\n",
    "collection = client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "print(f\"‚úÖ Collection '{COLLECTION_NAME}' ready\")\n",
    "print(f\"üìä Current collection size: {collection.count()} verses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "print(\"üîÑ Loading embedding model...\")\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f\"‚úÖ Embedding model loaded: {EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load All Verses into Vector Database\n",
    "\n",
    "This will fetch all verses from the API and store them with semantic embeddings.\n",
    "\n",
    "**Note:** This takes about 5-10 minutes to complete. Run it once, and the data will be persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we need to load data\n",
    "current_count = collection.count()\n",
    "expected_count = sum(CHAPTER_VERSES.values())\n",
    "\n",
    "if current_count >= expected_count:\n",
    "    print(f\"‚úÖ Database already contains {current_count} verses. Skipping data load.\")\n",
    "    print(\"   To reload data, delete the collection in the previous cell.\")\n",
    "else:\n",
    "    print(f\"üîÑ Loading data into vector database...\")\n",
    "    print(f\"   Current: {current_count} verses, Expected: {expected_count} verses\\n\")\n",
    "    \n",
    "    # Track statistics\n",
    "    total_verses = 0\n",
    "    failed_verses = []\n",
    "    \n",
    "    # Loop through all chapters\n",
    "    for chapter_num in range(1, 19):\n",
    "        max_verses = CHAPTER_VERSES[chapter_num]\n",
    "        chapter_verses_loaded = 0\n",
    "        \n",
    "        # Progress bar for this chapter\n",
    "        pbar = tqdm(range(1, max_verses + 1), \n",
    "                   desc=f\"Chapter {chapter_num:2d}\",\n",
    "                   unit=\"verse\")\n",
    "        \n",
    "        for verse_num in pbar:\n",
    "            # Check if verse already exists\n",
    "            verse_id = f\"{chapter_num}.{verse_num}\"\n",
    "            try:\n",
    "                existing = collection.get(ids=[verse_id])\n",
    "                if existing['ids']:\n",
    "                    chapter_verses_loaded += 1\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Fetch verse from API\n",
    "            data = fetch_verse(chapter_num, verse_num)\n",
    "            \n",
    "            if data is None:\n",
    "                failed_verses.append(f\"{chapter_num}.{verse_num}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract and structure data\n",
    "            verse_data = extract_verse_data(data, chapter_num, verse_num)\n",
    "            \n",
    "            # Add to vector database\n",
    "            collection.add(\n",
    "                ids=[verse_data[\"id\"]],\n",
    "                documents=[verse_data[\"search_text\"]],\n",
    "                metadatas=[verse_data[\"metadata\"]]\n",
    "            )\n",
    "            \n",
    "            chapter_verses_loaded += 1\n",
    "            total_verses += 1\n",
    "            \n",
    "            # Small delay to be respectful to the API\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        pbar.close()\n",
    "        print(f\"   ‚úÖ Chapter {chapter_num}: {chapter_verses_loaded}/{max_verses} verses loaded\")\n",
    "    \n",
    "    print(f\"\\nüéâ Data loading complete!\")\n",
    "    print(f\"   Total verses in database: {collection.count()}\")\n",
    "    print(f\"   New verses loaded: {total_verses}\")\n",
    "    \n",
    "    if failed_verses:\n",
    "        print(f\"   ‚ö†Ô∏è Failed to load {len(failed_verses)} verses: {failed_verses[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Search Function\n",
    "\n",
    "Query the vector database to find relevant slokas based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_slokas(query: str, n_results: int = 5, chapter_filter: Optional[int] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Search for relevant Bhagavad Gita verses based on semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        query: User's problem or question\n",
    "        n_results: Number of top results to return\n",
    "        chapter_filter: Optional chapter number to filter results (1-18)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing search results with metadata\n",
    "    \"\"\"\n",
    "    # Build where clause for filtering\n",
    "    where_clause = None\n",
    "    if chapter_filter:\n",
    "        where_clause = {\"chapter\": chapter_filter}\n",
    "    \n",
    "    # Perform semantic search\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        where=where_clause\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def format_search_results(results: Dict, show_commentary: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Format search results for display.\n",
    "    \n",
    "    Args:\n",
    "        results: Results from search_slokas()\n",
    "        show_commentary: Whether to include commentary in output\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with verse information\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    \n",
    "    for i, metadata in enumerate(results['metadatas'][0]):\n",
    "        verse_id = metadata['verse_id']\n",
    "        chapter = metadata['chapter']\n",
    "        verse = metadata['verse']\n",
    "        \n",
    "        output.append(f\"\\n{'='*80}\")\n",
    "        output.append(f\"üìñ Bhagavad Gita {verse_id} (Chapter {chapter}, Verse {verse})\")\n",
    "        output.append(f\"{'='*80}\")\n",
    "        \n",
    "        # Sanskrit\n",
    "        output.append(f\"\\nüïâÔ∏è **Sanskrit:**\\n{metadata['sanskrit']}\")\n",
    "        \n",
    "        # Transliteration\n",
    "        if metadata.get('transliteration'):\n",
    "            output.append(f\"\\nüìù **Transliteration:**\\n{metadata['transliteration']}\")\n",
    "        \n",
    "        # Translation\n",
    "        output.append(f\"\\nüåç **Translation:**\\n{metadata['translation']}\")\n",
    "        \n",
    "        # Commentary (optional)\n",
    "        if show_commentary and metadata.get('commentary'):\n",
    "            output.append(f\"\\nüí° **Commentary:**\\n{metadata['commentary'][:300]}...\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Search functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Semantic Search\n",
    "\n",
    "Let's test the search functionality with a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test search\n",
    "test_query = \"How to overcome fear and anxiety?\"\n",
    "\n",
    "print(f\"üîç Searching for: '{test_query}'\\n\")\n",
    "results = search_slokas(test_query, n_results=3)\n",
    "\n",
    "print(format_search_results(results, show_commentary=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM Integration\n",
    "\n",
    "Generate wisdom-based solutions using retrieved slokas and an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response_ollama(prompt: str, base_url: str = OLLAMA_BASE_URL, model: str = OLLAMA_MODEL) -> str:\n",
    "    \"\"\"\n",
    "    Get response from local Ollama LLM running in Kubernetes.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The formatted prompt\n",
    "        base_url: Ollama base URL\n",
    "        model: Model name to use\n",
    "    \n",
    "    Returns:\n",
    "        LLM response string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configure Ollama client to use Kubernetes service\n",
    "        client = ollama.Client(host=base_url)\n",
    "        \n",
    "        response = client.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are a compassionate spiritual guide well-versed in the Bhagavad Gita.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            options={\n",
    "                \"temperature\": 0.7,\n",
    "                \"num_predict\": 800\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response['message']['content']\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Error calling Ollama API at {base_url}: {str(e)}\\n\\nPlease ensure:\\n1. Ollama is running in your Kubernetes cluster\\n2. The service is accessible at {base_url}\\n3. Model '{model}' is available\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ Ollama LLM integration ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(problem: str, slokas_data: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt for the LLM combining the user's problem and retrieved slokas.\n",
    "    \n",
    "    Args:\n",
    "        problem: User's problem or question\n",
    "        slokas_data: Results from semantic search\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    # Extract slokas information\n",
    "    slokas = []\n",
    "    for metadata in slokas_data['metadatas'][0]:\n",
    "        sloka_info = f\"\"\"\n",
    "Verse {metadata['verse_id']}:\n",
    "Sanskrit: {metadata['sanskrit']}\n",
    "Translation: {metadata['translation']}\n",
    "Commentary: {metadata.get('commentary', 'N/A')}\n",
    "\"\"\"\n",
    "        slokas.append(sloka_info)\n",
    "    \n",
    "    slokas_text = \"\\n---\\n\".join(slokas)\n",
    "    \n",
    "    prompt = f\"\"\"You are a wise spiritual guide knowledgeable in the Bhagavad Gita. A person has come to you with the following problem:\n",
    "\n",
    "**Problem:** {problem}\n",
    "\n",
    "Based on your deep understanding of the Bhagavad Gita, you have identified the following relevant verses:\n",
    "\n",
    "{slokas_text}\n",
    "\n",
    "**Instructions:**\n",
    "1. Provide a compassionate and insightful response to the person's problem\n",
    "2. Reference specific verses (by chapter.verse number) that are relevant\n",
    "3. Explain the wisdom from these verses in a way that directly addresses their concern\n",
    "4. Offer practical guidance based on the teachings\n",
    "5. Keep your response concise (250-400 words) but meaningful\n",
    "6. Use a warm, supportive tone\n",
    "\n",
    "Please provide your guidance:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "print(\"‚úÖ Prompt engineering function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete RAG Pipeline\n",
    "\n",
    "Combine everything into a single function that takes a problem and returns a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_problem_with_gita(problem: str, \n",
    "                            n_slokas: int = 5, \n",
    "                            show_slokas: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: Search for slokas + Generate solution using local Ollama.\n",
    "    \n",
    "    Args:\n",
    "        problem: User's problem or question\n",
    "        n_slokas: Number of slokas to retrieve\n",
    "        show_slokas: Whether to display retrieved slokas\n",
    "    \n",
    "    Returns:\n",
    "        Formatted output with slokas and LLM-generated solution\n",
    "    \"\"\"\n",
    "    print(\"üîç Step 1: Searching for relevant slokas...\\n\")\n",
    "    \n",
    "    # Search for relevant slokas\n",
    "    slokas_data = search_slokas(problem, n_results=n_slokas)\n",
    "    \n",
    "    if show_slokas:\n",
    "        print(format_search_results(slokas_data, show_commentary=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Create prompt\n",
    "    print(\"ü§ñ Step 2: Generating wisdom-based solution with Ollama...\\n\")\n",
    "    prompt = create_prompt(problem, slokas_data)\n",
    "    \n",
    "    # Get LLM response from local Ollama\n",
    "    solution = get_llm_response_ollama(prompt)\n",
    "    \n",
    "    # Format final output\n",
    "    output = f\"\"\"\n",
    "{'='*80}\n",
    "üí° GUIDANCE FROM THE BHAGAVAD GITA\n",
    "{'='*80}\n",
    "\n",
    "{solution}\n",
    "\n",
    "{'='*80}\n",
    "üïâÔ∏è May the wisdom of the Gita guide your path\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "print(\"‚úÖ Complete RAG pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Demo\n",
    "\n",
    "Try the complete system with real problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Career Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem1 = \"\"\"\n",
    "I'm confused about my career path. I have a stable job, but I'm not passionate about it. \n",
    "My parents want me to continue in this field, but I dream of doing something different. \n",
    "I'm afraid of making the wrong choice and disappointing everyone. What should I do?\n",
    "\"\"\"\n",
    "\n",
    "response = solve_problem_with_gita(problem1, n_slokas=4,  show_slokas=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Managing Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem2 = \"\"\"\n",
    "I get angry very easily, especially when things don't go my way at work. \n",
    "This anger is affecting my relationships and my peace of mind. \n",
    "How can I control my anger and remain calm?\n",
    "\"\"\"\n",
    "\n",
    "response = solve_problem_with_gita(problem2, n_slokas=4,  show_slokas=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Dealing with Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem3 = \"\"\"\n",
    "I recently lost a loved one and I'm struggling to cope with the grief. \n",
    "I feel lost and don't know how to move forward. \n",
    "How can I find peace and accept this loss?\n",
    "\"\"\"\n",
    "\n",
    "response = solve_problem_with_gita(problem3, n_slokas=5,  show_slokas=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Your Own Problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your own problem here\n",
    "my_problem = \"\"\"\n",
    "I feel overwhelmed by all my responsibilities - work, family, health. \n",
    "I don't have time for myself and feel burnt out. How can I find balance?\n",
    "\"\"\"\n",
    "\n",
    "response = solve_problem_with_gita(my_problem, n_slokas=4,  show_slokas=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Queries\n",
    "\n",
    "Some helper functions for exploring the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verse_by_id(chapter: int, verse: int) -> str:\n",
    "    \"\"\"\n",
    "    Get a specific verse by chapter and verse number.\n",
    "    \"\"\"\n",
    "    verse_id = f\"{chapter}.{verse}\"\n",
    "    result = collection.get(ids=[verse_id])\n",
    "    \n",
    "    if result['ids']:\n",
    "        metadata = result['metadatas'][0]\n",
    "        output = f\"\"\"\n",
    "üìñ Bhagavad Gita {verse_id}\n",
    "{'='*80}\n",
    "\n",
    "üïâÔ∏è Sanskrit:\n",
    "{metadata['sanskrit']}\n",
    "\n",
    "üåç Translation:\n",
    "{metadata['translation']}\n",
    "\n",
    "üí° Commentary:\n",
    "{metadata.get('commentary', 'N/A')}\n",
    "\"\"\"\n",
    "        return output\n",
    "    else:\n",
    "        return f\"‚ùå Verse {verse_id} not found in database.\"\n",
    "\n",
    "\n",
    "# Example: Get the famous verse 2.47\n",
    "print(get_verse_by_id(2, 47))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_chapter(query: str, chapter: int, n_results: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Search within a specific chapter only.\n",
    "    \"\"\"\n",
    "    print(f\"üîç Searching in Chapter {chapter} for: '{query}'\\n\")\n",
    "    results = search_slokas(query, n_results=n_results, chapter_filter=chapter)\n",
    "    return format_search_results(results, show_commentary=False)\n",
    "\n",
    "\n",
    "# Example: Search for verses about dharma in Chapter 2\n",
    "print(search_by_chapter(\"duty and dharma\", chapter=2, n_results=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Database Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics about the database\n",
    "total_verses = collection.count()\n",
    "\n",
    "print(\"üìä Bhagavad Gita Vector Database Statistics\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total verses in database: {total_verses}\")\n",
    "print(f\"Expected total verses: {sum(CHAPTER_VERSES.values())}\")\n",
    "print(f\"\\nChapter-wise verse count:\")\n",
    "\n",
    "for chapter in range(1, 19):\n",
    "    count = len(collection.get(where={\"chapter\": chapter})['ids'])\n",
    "    expected = CHAPTER_VERSES[chapter]\n",
    "    status = \"‚úÖ\" if count == expected else \"‚ö†Ô∏è\"\n",
    "    print(f\"  Chapter {chapter:2d}: {count:2d}/{expected:2d} verses {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "### Enhancements You Can Make:\n",
    "\n",
    "1. **Web Interface**: Build a Streamlit or Gradio interface for easier interaction\n",
    "2. **Multiple Languages**: Add support for Hindi, Sanskrit responses\n",
    "3. **Context Memory**: Implement conversation history for follow-up questions\n",
    "4. **Fine-tuning**: Fine-tune the embedding model on Gita-specific corpus\n",
    "5. **Audio Output**: Add text-to-speech for verse recitation\n",
    "6. **Daily Verse**: Create a feature to get a random verse daily\n",
    "7. **Verse Comparison**: Compare interpretations from different commentators\n",
    "8. **Mind Map**: Visualize connections between verses on similar topics\n",
    "\n",
    "### Configuration Tips:\n",
    "\n",
    "1. **API Keys**: Set environment variables for security:\n",
    "   ```bash\n",
    "   export GROQ_API_KEY=\"your_key_here\"\n",
    "   export OPENAI_API_KEY=\"your_key_here\"\n",
    "   ```\n",
    "\n",
    "2. **Local LLM**: Use Ollama for completely local operation:\n",
    "   ```bash\n",
    "   ollama pull llama3.2\n",
    "   ```\n",
    "\n",
    "3. **Tuning Search**: Adjust `n_slokas` parameter (3-7 works best)\n",
    "\n",
    "4. **Database Reset**: To reload data from scratch, delete the `bg_vector_db` folder\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [Bhagavad Gita API Docs](https://github.com/vedicscriptures/bhagavad-gita)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [Groq API](https://console.groq.com/)\n",
    "\n",
    "üïâÔ∏è **May this tool help bring the timeless wisdom of the Bhagavad Gita to those who seek it!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

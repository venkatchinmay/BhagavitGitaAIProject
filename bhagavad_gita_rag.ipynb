{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bhagavad Gita AI Assistant ğŸ•‰ï¸\n",
    "\n",
    "This notebook implements a RAG (Retrieval-Augmented Generation) system that:\n",
    "1. **Loads** all 700+ verses from the Bhagavad Gita API\n",
    "2. **Stores** them in a vector database (ChromaDB) with semantic embeddings\n",
    "3. **Retrieves** relevant slokas based on user problems\n",
    "4. **Generates** wisdom-based solutions using an LLM\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "User Problem â†’ Semantic Search â†’ Top-K Slokas â†’ LLM Reasoning â†’ Solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "\n",
    "Install dependencies first:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ollama\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Database path: ./bg_vector_db\n",
      "ğŸ¤– Embedding model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "ğŸ¦™ Ollama URL: http://localhost:11434\n",
      "ğŸ§  Ollama model: phi3.5:latest\n",
      "ğŸ“š Total verses to load: 701\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DB_PATH = \"./bg_vector_db\"\n",
    "COLLECTION_NAME = \"bhagavad_gita\"\n",
    "EMBEDDING_MODEL = 'paraphrase-multilingual-MiniLM-L12-v2'  # Supports English and Sanskrit\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 2  # seconds\n",
    "\n",
    "# Chapter structure (number of verses per chapter)\n",
    "# Source: https://github.com/vedicscriptures/bhagavad-gita\n",
    "CHAPTER_VERSES = {\n",
    "    1: 47, 2: 72, 3: 43, 4: 42, 5: 29, 6: 47,\n",
    "    7: 30, 8: 28, 9: 34, 10: 42, 11: 55, 12: 20,\n",
    "    13: 35, 14: 27, 15: 20, 16: 24, 17: 28, 18: 78\n",
    "}\n",
    "\n",
    "# Ollama Configuration (running in Kubernetes)\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"phi3.5:latest\")\n",
    "print(f\"ğŸ“ Database path: {DB_PATH}\")\n",
    "print(f\"ğŸ¤– Embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"ğŸ¦™ Ollama URL: {OLLAMA_BASE_URL}\")\n",
    "print(f\"ğŸ§  Ollama model: {OLLAMA_MODEL}\")\n",
    "print(f\"ğŸ“š Total verses to load: {sum(CHAPTER_VERSES.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading Functions\n",
    "\n",
    "Enhanced version with:\n",
    "- Error handling and retries\n",
    "- Progress tracking\n",
    "- Rich metadata extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "def fetch_verse(chapter: int, verse: int, max_retries: int = MAX_RETRIES) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch a single verse from the Bhagavad Gita API with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        chapter: Chapter number (1-18)\n",
    "        verse: Verse number\n",
    "        max_retries: Maximum number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing verse data or None if failed\n",
    "    \"\"\"\n",
    "    url = f\"https://vedicscriptures.github.io/slok/{chapter}/{verse}/\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            elif response.status_code == 404:\n",
    "                # Verse doesn't exist, don't retry\n",
    "                return None\n",
    "            else:\n",
    "                # Other error, retry\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                continue\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                print(f\"\\nâŒ Failed to fetch {chapter}.{verse} after {max_retries} attempts: {e}\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_verse_data(data: Dict, chapter: int, verse: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract and structure relevant data from API response.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with structured verse data\n",
    "    \"\"\"\n",
    "    # Build comprehensive search text combining all translations and commentaries\n",
    "    search_components = []\n",
    "    \n",
    "    # Add English translation (Sivananda)\n",
    "    siva_translation = data.get('siva', {}).get('et', '')\n",
    "    if siva_translation:\n",
    "        search_components.append(siva_translation)\n",
    "    \n",
    "    # Add Sivananda commentary\n",
    "    siva_commentary = data.get('siva', {}).get('ec', '')\n",
    "    if siva_commentary:\n",
    "        search_components.append(siva_commentary)\n",
    "    \n",
    "    # Add Chinmayananda commentary (often very insightful)\n",
    "    chinmay_commentary = data.get('chinmay', {}).get('ec', '')\n",
    "    if chinmay_commentary:\n",
    "        search_components.append(chinmay_commentary)\n",
    "    \n",
    "    # Add Purohit Swami translation (simpler English)\n",
    "    purohit_translation = data.get('purohit', {}).get('et', '')\n",
    "    if purohit_translation:\n",
    "        search_components.append(purohit_translation)\n",
    "    \n",
    "    # Combine all components\n",
    "    search_text = \" \".join(search_components)\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = {\n",
    "        \"chapter\": chapter,\n",
    "        \"verse\": verse,\n",
    "        \"verse_id\": f\"{chapter}.{verse}\",\n",
    "        \"sanskrit\": data.get(\"slok\", \"\"),\n",
    "        \"transliteration\": data.get(\"transliteration\", \"\"),\n",
    "        \"translation\": siva_translation,\n",
    "        \"commentary\": siva_commentary[:500] if siva_commentary else \"\",  # Truncate for metadata\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"id\": f\"{chapter}.{verse}\",\n",
    "        \"search_text\": search_text,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"âœ… Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Vector Database\n",
    "\n",
    "Setup ChromaDB with persistent storage and embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Initializing ChromaDB...\n",
      "âœ… Collection 'bhagavad_gita' ready\n",
      "ğŸ“Š Current collection size: 0 verses\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB client\n",
    "print(\"ğŸ”„ Initializing ChromaDB...\")\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "# Delete existing collection if you want to reload data\n",
    "# Uncomment the following lines to reset the database\n",
    "# try:\n",
    "#     client.delete_collection(name=COLLECTION_NAME)\n",
    "#     print(\"ğŸ—‘ï¸ Deleted existing collection\")\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# Get or create collection\n",
    "collection = client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "print(f\"âœ… Collection '{COLLECTION_NAME}' ready\")\n",
    "print(f\"ğŸ“Š Current collection size: {collection.count()} verses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading embedding model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60dca6a68e18440891bf5bb79e4d5808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53de471be654b648595dfd6ffe71e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cdb753ac9e940f1833ee9f4b6e314b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a2daac3a634c9d9567235d983fff92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75f3c3720c04d729dd99da6f81c4cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2350efbe2e364ab5bd15c96c5d9a3e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c7c9976ab44cd188b3e301a94d4aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202cf744b7714fe5a742227718d9fd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/526 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc02bb208454999a841ba98a97227f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5005afc861e407887bb480b5d6a9a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83982d25413640a6b2e832a9901a137e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chinmay/git_projects/BhagavitGitaAIProject/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GeForce MX330 which is of cuda capability 6.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n",
      "/home/chinmay/git_projects/BhagavitGitaAIProject/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  queued_call()\n",
      "/home/chinmay/git_projects/BhagavitGitaAIProject/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "NVIDIA GeForce MX330 with CUDA capability sm_61 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the NVIDIA GeForce MX330 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  queued_call()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embedding model loaded: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "print(\"ğŸ”„ Loading embedding model...\")\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f\"âœ… Embedding model loaded: {EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load All Verses into Vector Database\n",
    "\n",
    "This will fetch all verses from the API and store them with semantic embeddings.\n",
    "\n",
    "**Note:** This takes about 5-10 minutes to complete. Run it once, and the data will be persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading data into vector database...\n",
      "   Current: 0 verses, Expected: 701 verses\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4257654fa3a743d5a0e00ec942d3c3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter  1:   0%|          | 0/47 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chinmay/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79.3M/79.3M [01:34<00:00, 885kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 1: 47/47 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0248a8caed9f459daad81622f4f2e58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter  2:   0%|          | 0/72 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 2: 72/72 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d749f347e5d481d84f8158d510495ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter  3:   0%|          | 0/43 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 3: 43/43 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ee1e77b53348e9bd03b449049fd362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter  4:   0%|          | 0/42 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 4: 42/42 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bec95a74f347aeaa00b0bdbd079e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter  5:   0%|          | 0/29 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 5: 29/29 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c426370d0d9f4787884d8a802dc84ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter  6:   0%|          | 0/47 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 6: 47/47 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d857000352745458fdae52b066bff01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter  7:   0%|          | 0/30 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 7: 30/30 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e92b75f3604755abbccd6d52f3dd57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter  8:   0%|          | 0/28 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 8: 28/28 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5088824907ec430d8a71083facc2b1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter  9:   0%|          | 0/34 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 9: 34/34 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef354bfd855412da44b9547ee7a3936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter 10:   0%|          | 0/42 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 10: 42/42 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ca8a6d258449f39df42b7caca5ea12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter 11:   0%|          | 0/55 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 11: 55/55 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9052c98b99614009ba46ae50cfedd022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter 12:   0%|          | 0/20 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 12: 20/20 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0018fe25a514da9a7220a064b09a1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter 13:   0%|          | 0/35 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 13: 35/35 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62422b7feca4c1db4b53e0b87ca2099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter 14:   0%|          | 0/27 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 14: 27/27 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab90be67c559443698cea046644c6f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter 15:   0%|          | 0/20 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 15: 20/20 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e011a1c9e1465481efb71558d6c719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter 16:   0%|          | 0/24 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 16: 24/24 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9d47d8095e49c3b539295a9967accf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter 17:   0%|          | 0/28 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 17: 28/28 verses loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd563ffc45464ee4806fd92611848382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chapter 18:   0%|          | 0/78 [00:00<?, ?verse/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Chapter 18: 78/78 verses loaded\n",
      "\n",
      "ğŸ‰ Data loading complete!\n",
      "   Total verses in database: 701\n",
      "   New verses loaded: 701\n"
     ]
    }
   ],
   "source": [
    "# Check if we need to load data\n",
    "current_count = collection.count()\n",
    "expected_count = sum(CHAPTER_VERSES.values())\n",
    "\n",
    "if current_count >= expected_count:\n",
    "    print(f\"âœ… Database already contains {current_count} verses. Skipping data load.\")\n",
    "    print(\"   To reload data, delete the collection in the previous cell.\")\n",
    "else:\n",
    "    print(f\"ğŸ”„ Loading data into vector database...\")\n",
    "    print(f\"   Current: {current_count} verses, Expected: {expected_count} verses\\n\")\n",
    "    \n",
    "    # Track statistics\n",
    "    total_verses = 0\n",
    "    failed_verses = []\n",
    "    \n",
    "    # Loop through all chapters\n",
    "    for chapter_num in range(1, 19):\n",
    "        max_verses = CHAPTER_VERSES[chapter_num]\n",
    "        chapter_verses_loaded = 0\n",
    "        \n",
    "        # Progress bar for this chapter\n",
    "        pbar = tqdm(range(1, max_verses + 1), \n",
    "                   desc=f\"Chapter {chapter_num:2d}\",\n",
    "                   unit=\"verse\")\n",
    "        \n",
    "        for verse_num in pbar:\n",
    "            # Check if verse already exists\n",
    "            verse_id = f\"{chapter_num}.{verse_num}\"\n",
    "            try:\n",
    "                existing = collection.get(ids=[verse_id])\n",
    "                if existing['ids']:\n",
    "                    chapter_verses_loaded += 1\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Fetch verse from API\n",
    "            data = fetch_verse(chapter_num, verse_num)\n",
    "            \n",
    "            if data is None:\n",
    "                failed_verses.append(f\"{chapter_num}.{verse_num}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract and structure data\n",
    "            verse_data = extract_verse_data(data, chapter_num, verse_num)\n",
    "            \n",
    "            # Add to vector database\n",
    "            collection.add(\n",
    "                ids=[verse_data[\"id\"]],\n",
    "                documents=[verse_data[\"search_text\"]],\n",
    "                metadatas=[verse_data[\"metadata\"]]\n",
    "            )\n",
    "            \n",
    "            chapter_verses_loaded += 1\n",
    "            total_verses += 1\n",
    "            \n",
    "            # Small delay to be respectful to the API\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        pbar.close()\n",
    "        print(f\"   âœ… Chapter {chapter_num}: {chapter_verses_loaded}/{max_verses} verses loaded\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Data loading complete!\")\n",
    "    print(f\"   Total verses in database: {collection.count()}\")\n",
    "    print(f\"   New verses loaded: {total_verses}\")\n",
    "    \n",
    "    if failed_verses:\n",
    "        print(f\"   âš ï¸ Failed to load {len(failed_verses)} verses: {failed_verses[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Search Function\n",
    "\n",
    "Query the vector database to find relevant slokas based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Search functions defined\n"
     ]
    }
   ],
   "source": [
    "def search_slokas(query: str, n_results: int = 5, chapter_filter: Optional[int] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Search for relevant Bhagavad Gita verses based on semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        query: User's problem or question\n",
    "        n_results: Number of top results to return\n",
    "        chapter_filter: Optional chapter number to filter results (1-18)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing search results with metadata\n",
    "    \"\"\"\n",
    "    # Build where clause for filtering\n",
    "    where_clause = None\n",
    "    if chapter_filter:\n",
    "        where_clause = {\"chapter\": chapter_filter}\n",
    "    \n",
    "    # Perform semantic search\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        where=where_clause\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def format_search_results(results: Dict, show_commentary: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Format search results for display.\n",
    "    \n",
    "    Args:\n",
    "        results: Results from search_slokas()\n",
    "        show_commentary: Whether to include commentary in output\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with verse information\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    \n",
    "    for i, metadata in enumerate(results['metadatas'][0]):\n",
    "        verse_id = metadata['verse_id']\n",
    "        chapter = metadata['chapter']\n",
    "        verse = metadata['verse']\n",
    "        \n",
    "        output.append(f\"\\n{'='*80}\")\n",
    "        output.append(f\"ğŸ“– Bhagavad Gita {verse_id} (Chapter {chapter}, Verse {verse})\")\n",
    "        output.append(f\"{'='*80}\")\n",
    "        \n",
    "        # Sanskrit\n",
    "        output.append(f\"\\nğŸ•‰ï¸ **Sanskrit:**\\n{metadata['sanskrit']}\")\n",
    "        \n",
    "        # Transliteration\n",
    "        if metadata.get('transliteration'):\n",
    "            output.append(f\"\\nğŸ“ **Transliteration:**\\n{metadata['transliteration']}\")\n",
    "        \n",
    "        # Translation\n",
    "        output.append(f\"\\nğŸŒ **Translation:**\\n{metadata['translation']}\")\n",
    "        \n",
    "        # Commentary (optional)\n",
    "        if show_commentary and metadata.get('commentary'):\n",
    "            output.append(f\"\\nğŸ’¡ **Commentary:**\\n{metadata['commentary'][:300]}...\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "print(\"âœ… Search functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Semantic Search\n",
    "\n",
    "Let's test the search functionality with a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Searching for: 'How to overcome fear and anxiety?'\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– Bhagavad Gita 10.4 (Chapter 10, Verse 4)\n",
      "================================================================================\n",
      "\n",
      "ğŸ•‰ï¸ **Sanskrit:**\n",
      "à¤¬à¥à¤¦à¥à¤§à¤¿à¤°à¥à¤œà¥à¤à¤¾à¤¨à¤®à¤¸à¤®à¥à¤®à¥‹à¤¹à¤ƒ à¤•à¥à¤·à¤®à¤¾ à¤¸à¤¤à¥à¤¯à¤‚ à¤¦à¤®à¤ƒ à¤¶à¤®à¤ƒ |\n",
      "à¤¸à¥à¤–à¤‚ à¤¦à¥à¤ƒà¤–à¤‚ à¤­à¤µà¥‹à¤½à¤­à¤¾à¤µà¥‹ à¤­à¤¯à¤‚ à¤šà¤¾à¤­à¤¯à¤®à¥‡à¤µ à¤š ||à¥§à¥¦-à¥ª||\n",
      "\n",
      "ğŸ“ **Transliteration:**\n",
      "buddhirjÃ±Änamasammohaá¸¥ ká¹£amÄ satyaá¹ƒ damaá¸¥ Å›amaá¸¥ .\n",
      "sukhaá¹ƒ duá¸¥khaá¹ƒ bhavo.abhÄvo bhayaá¹ƒ cÄbhayameva ca ||10-4||\n",
      "\n",
      "ğŸŒ **Translation:**\n",
      "10.4 Intellect, wisdom, non-delusion, forgiveness, truth, self-restraint, calmness, happiness, pain, existence or birth, non-existence or death, fear and also fearlessness.\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– Bhagavad Gita 11.49 (Chapter 11, Verse 49)\n",
      "================================================================================\n",
      "\n",
      "ğŸ•‰ï¸ **Sanskrit:**\n",
      "à¤®à¤¾ à¤¤à¥‡ à¤µà¥à¤¯à¤¥à¤¾ à¤®à¤¾ à¤š à¤µà¤¿à¤®à¥‚à¤¢à¤­à¤¾à¤µà¥‹\n",
      "à¤¦à¥ƒà¤·à¥à¤Ÿà¥à¤µà¤¾ à¤°à¥‚à¤ªà¤‚ à¤˜à¥‹à¤°à¤®à¥€à¤¦à¥ƒà¤™à¥à¤®à¤®à¥‡à¤¦à¤®à¥ |\n",
      "à¤µà¥à¤¯à¤ªà¥‡à¤¤à¤­à¥€à¤ƒ à¤ªà¥à¤°à¥€à¤¤à¤®à¤¨à¤¾à¤ƒ à¤ªà¥à¤¨à¤¸à¥à¤¤à¥à¤µà¤‚\n",
      "à¤¤à¤¦à¥‡à¤µ à¤®à¥‡ à¤°à¥‚à¤ªà¤®à¤¿à¤¦à¤‚ à¤ªà¥à¤°à¤ªà¤¶à¥à¤¯ ||à¥§à¥§-à¥ªà¥¯||\n",
      "\n",
      "ğŸ“ **Transliteration:**\n",
      "mÄ te vyathÄ mÄ ca vimÅ«á¸habhÄvo dá¹›á¹£á¹­vÄ rÅ«paá¹ƒ ghoramÄ«dá¹›á¹…mamedam .\n",
      "vyapetabhÄ«á¸¥ prÄ«tamanÄá¸¥ punastvaá¹ƒ tadeva me rÅ«pamidaá¹ƒ prapaÅ›ya ||11-49||\n",
      "\n",
      "ğŸŒ **Translation:**\n",
      "11.49 Be not afraid, nor bewildered on seeing such a teriible form of Mine as this; with thy fear dispelled and with a gladdened heart, now behold again this former form of Mine.\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– Bhagavad Gita 12.15 (Chapter 12, Verse 15)\n",
      "================================================================================\n",
      "\n",
      "ğŸ•‰ï¸ **Sanskrit:**\n",
      "à¤¯à¤¸à¥à¤®à¤¾à¤¨à¥à¤¨à¥‹à¤¦à¥à¤µà¤¿à¤œà¤¤à¥‡ à¤²à¥‹à¤•à¥‹ à¤²à¥‹à¤•à¤¾à¤¨à¥à¤¨à¥‹à¤¦à¥à¤µà¤¿à¤œà¤¤à¥‡ à¤š à¤¯à¤ƒ |\n",
      "à¤¹à¤°à¥à¤·à¤¾à¤®à¤°à¥à¤·à¤­à¤¯à¥‹à¤¦à¥à¤µà¥‡à¤—à¥ˆà¤°à¥à¤®à¥à¤•à¥à¤¤à¥‹ à¤¯à¤ƒ à¤¸ à¤š à¤®à¥‡ à¤ªà¥à¤°à¤¿à¤¯à¤ƒ ||à¥§à¥¨-à¥§à¥«||\n",
      "\n",
      "ğŸ“ **Transliteration:**\n",
      "yasmÄnnodvijate loko lokÄnnodvijate ca yaá¸¥ .\n",
      "hará¹£Ämará¹£abhayodvegairmukto yaá¸¥ sa ca me priyaá¸¥ ||12-15||\n",
      "\n",
      "ğŸŒ **Translation:**\n",
      "12.15 He by whom the world is not agitated and who cannot be agitated by the world, and who is freed from joy, anger, fear and anxiety  he is dear to Me.\n"
     ]
    }
   ],
   "source": [
    "# Test search\n",
    "test_query = \"How to overcome fear and anxiety?\"\n",
    "\n",
    "print(f\"ğŸ” Searching for: '{test_query}'\\n\")\n",
    "results = search_slokas(test_query, n_results=3)\n",
    "\n",
    "print(format_search_results(results, show_commentary=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM Integration\n",
    "\n",
    "Generate wisdom-based solutions using retrieved slokas and an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ollama LLM integration ready!\n"
     ]
    }
   ],
   "source": [
    "def get_llm_response_ollama(prompt: str, base_url: str = OLLAMA_BASE_URL, model: str = OLLAMA_MODEL) -> str:\n",
    "    \"\"\"\n",
    "    Get response from local Ollama LLM running in Kubernetes.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The formatted prompt\n",
    "        base_url: Ollama base URL\n",
    "        model: Model name to use\n",
    "    \n",
    "    Returns:\n",
    "        LLM response string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configure Ollama client to use Kubernetes service\n",
    "        client = ollama.Client(host=base_url)\n",
    "        \n",
    "        response = client.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are a compassionate spiritual guide well-versed in the Bhagavad Gita.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            options={\n",
    "                \"temperature\": 0.7,\n",
    "                \"num_predict\": 800\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response['message']['content']\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"âš ï¸ Error calling Ollama API at {base_url}: {str(e)}\\n\\nPlease ensure:\\n1. Ollama is running in your Kubernetes cluster\\n2. The service is accessible at {base_url}\\n3. Model '{model}' is available\"\n",
    "\n",
    "\n",
    "print(\"âœ… Ollama LLM integration ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prompt engineering function defined\n"
     ]
    }
   ],
   "source": [
    "def create_prompt(problem: str, slokas_data: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt for the LLM combining the user's problem and retrieved slokas.\n",
    "    \n",
    "    Args:\n",
    "        problem: User's problem or question\n",
    "        slokas_data: Results from semantic search\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    # Extract slokas information\n",
    "    slokas = []\n",
    "    for metadata in slokas_data['metadatas'][0]:\n",
    "        sloka_info = f\"\"\"\n",
    "Verse {metadata['verse_id']}:\n",
    "Sanskrit: {metadata['sanskrit']}\n",
    "Translation: {metadata['translation']}\n",
    "Commentary: {metadata.get('commentary', 'N/A')}\n",
    "\"\"\"\n",
    "        slokas.append(sloka_info)\n",
    "    \n",
    "    slokas_text = \"\\n---\\n\".join(slokas)\n",
    "    \n",
    "    prompt = f\"\"\"You are a wise spiritual guide knowledgeable in the Bhagavad Gita. A person has come to you with the following problem:\n",
    "\n",
    "**Problem:** {problem}\n",
    "\n",
    "Based on your deep understanding of the Bhagavad Gita, you have identified the following relevant verses:\n",
    "\n",
    "{slokas_text}\n",
    "\n",
    "**Instructions:**\n",
    "1. Provide a compassionate and insightful response to the person's problem\n",
    "2. Reference specific verses (by chapter.verse number) that are relevant\n",
    "3. Explain the wisdom from these verses in a way that directly addresses their concern\n",
    "4. Offer practical guidance based on the teachings\n",
    "5. Keep your response concise (250-400 words) but meaningful\n",
    "6. Use a warm, supportive tone\n",
    "\n",
    "Please provide your guidance:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "print(\"âœ… Prompt engineering function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete RAG Pipeline\n",
    "\n",
    "Combine everything into a single function that takes a problem and returns a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Complete RAG pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "def solve_problem_with_gita(problem: str, \n",
    "                            n_slokas: int = 5, \n",
    "                            show_slokas: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: Search for slokas + Generate solution using local Ollama.\n",
    "    \n",
    "    Args:\n",
    "        problem: User's problem or question\n",
    "        n_slokas: Number of slokas to retrieve\n",
    "        show_slokas: Whether to display retrieved slokas\n",
    "    \n",
    "    Returns:\n",
    "        Formatted output with slokas and LLM-generated solution\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” Step 1: Searching for relevant slokas...\\n\")\n",
    "    \n",
    "    # Search for relevant slokas\n",
    "    slokas_data = search_slokas(problem, n_results=n_slokas)\n",
    "    \n",
    "    if show_slokas:\n",
    "        print(format_search_results(slokas_data, show_commentary=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Create prompt\n",
    "    print(\"ğŸ¤– Step 2: Generating wisdom-based solution with Ollama...\\n\")\n",
    "    prompt = create_prompt(problem, slokas_data)\n",
    "    \n",
    "    # Get LLM response from local Ollama\n",
    "    solution = get_llm_response_ollama(prompt)\n",
    "    \n",
    "    # Format final output\n",
    "    output = f\"\"\"\n",
    "{'='*80}\n",
    "ğŸ’¡ GUIDANCE FROM THE BHAGAVAD GITA\n",
    "{'='*80}\n",
    "\n",
    "{solution}\n",
    "\n",
    "{'='*80}\n",
    "ğŸ•‰ï¸ May the wisdom of the Gita guide your path\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "print(\"âœ… Complete RAG pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Demo\n",
    "\n",
    "Try the complete system with real problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Career Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Step 1: Searching for relevant slokas...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– Bhagavad Gita 6.24 (Chapter 6, Verse 24)\n",
      "================================================================================\n",
      "\n",
      "ğŸ•‰ï¸ **Sanskrit:**\n",
      "à¤¸à¤™à¥à¤•à¤²à¥à¤ªà¤ªà¥à¤°à¤­à¤µà¤¾à¤¨à¥à¤•à¤¾à¤®à¤¾à¤‚à¤¸à¥à¤¤à¥à¤¯à¤•à¥à¤¤à¥à¤µà¤¾ à¤¸à¤°à¥à¤µà¤¾à¤¨à¤¶à¥‡à¤·à¤¤à¤ƒ |\n",
      "à¤®à¤¨à¤¸à¥ˆà¤µà¥‡à¤¨à¥à¤¦à¥à¤°à¤¿à¤¯à¤—à¥à¤°à¤¾à¤®à¤‚ à¤µà¤¿à¤¨à¤¿à¤¯à¤®à¥à¤¯ à¤¸à¤®à¤¨à¥à¤¤à¤¤à¤ƒ ||à¥¬-à¥¨à¥ª||\n",
      "\n",
      "ğŸ“ **Transliteration:**\n",
      "saá¹…kalpaprabhavÄnkÄmÄá¹ƒstyaktvÄ sarvÄnaÅ›eá¹£ataá¸¥ .\n",
      "manasaivendriyagrÄmaá¹ƒ viniyamya samantataá¸¥ ||6-24||\n",
      "\n",
      "ğŸŒ **Translation:**\n",
      "6.24 Abandoning without reserve all desires born of Sankalpa (thought and imagination) and completely restraining the whole group of the senses by the mind from all sides.\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– Bhagavad Gita 9.33 (Chapter 9, Verse 33)\n",
      "================================================================================\n",
      "\n",
      "ğŸ•‰ï¸ **Sanskrit:**\n",
      "à¤•à¤¿à¤‚ à¤ªà¥à¤¨à¤°à¥à¤¬à¥à¤°à¤¾à¤¹à¥à¤®à¤£à¤¾à¤ƒ à¤ªà¥à¤£à¥à¤¯à¤¾ à¤­à¤•à¥à¤¤à¤¾ à¤°à¤¾à¤œà¤°à¥à¤·à¤¯à¤¸à¥à¤¤à¤¥à¤¾ |\n",
      "à¤…à¤¨à¤¿à¤¤à¥à¤¯à¤®à¤¸à¥à¤–à¤‚ à¤²à¥‹à¤•à¤®à¤¿à¤®à¤‚ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¯ à¤­à¤œà¤¸à¥à¤µ à¤®à¤¾à¤®à¥ ||à¥¯-à¥©à¥©||\n",
      "\n",
      "ğŸ“ **Transliteration:**\n",
      "kiá¹ƒ punarbrÄhmaá¹‡Äá¸¥ puá¹‡yÄ bhaktÄ rÄjará¹£ayastathÄ .\n",
      "anityamasukhaá¹ƒ lokamimaá¹ƒ prÄpya bhajasva mÄm ||9-33||\n",
      "\n",
      "ğŸŒ **Translation:**\n",
      "9.33 How much more (easily) then the hold Brahmins and devoted royal saints (attain the goal); having come to this impermanent and unhappy world, do thou worship Me.\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– Bhagavad Gita 18.31 (Chapter 18, Verse 31)\n",
      "================================================================================\n",
      "\n",
      "ğŸ•‰ï¸ **Sanskrit:**\n",
      "à¤¯à¤¯à¤¾ à¤§à¤°à¥à¤®à¤®à¤§à¤°à¥à¤®à¤‚ à¤š à¤•à¤¾à¤°à¥à¤¯à¤‚ à¤šà¤¾à¤•à¤¾à¤°à¥à¤¯à¤®à¥‡à¤µ à¤š |\n",
      "à¤…à¤¯à¤¥à¤¾à¤µà¤¤à¥à¤ªà¥à¤°à¤œà¤¾à¤¨à¤¾à¤¤à¤¿ à¤¬à¥à¤¦à¥à¤§à¤¿à¤ƒ à¤¸à¤¾ à¤ªà¤¾à¤°à¥à¤¥ à¤°à¤¾à¤œà¤¸à¥€ ||à¥§à¥®-à¥©à¥§||\n",
      "\n",
      "ğŸ“ **Transliteration:**\n",
      "yayÄ dharmamadharmaá¹ƒ ca kÄryaá¹ƒ cÄkÄryameva ca .\n",
      "ayathÄvatprajÄnÄti buddhiá¸¥ sÄ pÄrtha rÄjasÄ« ||18-31||\n",
      "\n",
      "ğŸŒ **Translation:**\n",
      "18.31 That, by which one wrongly understands Dharma and Adharma and also what ought to be done and what ought not to be done  that intellect, O Arjuna, is Rajasic (passionate).\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– Bhagavad Gita 9.3 (Chapter 9, Verse 3)\n",
      "================================================================================\n",
      "\n",
      "ğŸ•‰ï¸ **Sanskrit:**\n",
      "à¤…à¤¶à¥à¤°à¤¦à¥à¤¦à¤§à¤¾à¤¨à¤¾à¤ƒ à¤ªà¥à¤°à¥à¤·à¤¾ à¤§à¤°à¥à¤®à¤¸à¥à¤¯à¤¾à¤¸à¥à¤¯ à¤ªà¤°à¤¨à¥à¤¤à¤ª |\n",
      "à¤…à¤ªà¥à¤°à¤¾à¤ªà¥à¤¯ à¤®à¤¾à¤‚ à¤¨à¤¿à¤µà¤°à¥à¤¤à¤¨à¥à¤¤à¥‡ à¤®à¥ƒà¤¤à¥à¤¯à¥à¤¸à¤‚à¤¸à¤¾à¤°à¤µà¤°à¥à¤¤à¥à¤®à¤¨à¤¿ ||à¥¯-à¥©||\n",
      "\n",
      "ğŸ“ **Transliteration:**\n",
      "aÅ›raddadhÄnÄá¸¥ puruá¹£Ä dharmasyÄsya parantapa .\n",
      "aprÄpya mÄá¹ƒ nivartante má¹›tyusaá¹ƒsÄravartmani ||9-3||\n",
      "\n",
      "ğŸŒ **Translation:**\n",
      "9.3 Those who have no faith in this Dharma (knowledge of the Self), O Parantapa (Arjuna), return to the path of this world of death without attaining Me.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ¤– Step 2: Generating wisdom-based solution with Ollama...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¡ GUIDANCE FROM THE BHAGAVAD GITA\n",
      "================================================================================\n",
      "\n",
      "âš ï¸ Error calling Ollama API at http://localhost:11434: Server disconnected without sending a response.\n",
      "\n",
      "Please ensure:\n",
      "1. Ollama is running in your Kubernetes cluster\n",
      "2. The service is accessible at http://localhost:11434\n",
      "3. Model 'phi3.5:latest' is available\n",
      "\n",
      "================================================================================\n",
      "ğŸ•‰ï¸ May the wisdom of the Gita guide your path\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "problem1 = \"\"\"\n",
    "I'm confused about my career path. I have a stable job, but I'm not passionate about it. \n",
    "My parents want me to continue in this field, but I dream of doing something different. \n",
    "I'm afraid of making the wrong choice and disappointing everyone. What should I do?\n",
    "\"\"\"\n",
    "\n",
    "response = solve_problem_with_gita(problem1, n_slokas=4,  show_slokas=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Managing Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem2 = \"\"\"\n",
    "I get angry very easily, especially when things don't go my way at work. \n",
    "This anger is affecting my relationships and my peace of mind. \n",
    "How can I control my anger and remain calm?\n",
    "\"\"\"\n",
    "\n",
    "response = solve_problem_with_gita(problem2, n_slokas=4,  show_slokas=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Dealing with Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem3 = \"\"\"\n",
    "I recently lost a loved one and I'm struggling to cope with the grief. \n",
    "I feel lost and don't know how to move forward. \n",
    "How can I find peace and accept this loss?\n",
    "\"\"\"\n",
    "\n",
    "response = solve_problem_with_gita(problem3, n_slokas=5,  show_slokas=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Your Own Problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your own problem here\n",
    "my_problem = \"\"\"\n",
    "I feel overwhelmed by all my responsibilities - work, family, health. \n",
    "I don't have time for myself and feel burnt out. How can I find balance?\n",
    "\"\"\"\n",
    "\n",
    "response = solve_problem_with_gita(my_problem, n_slokas=4,  show_slokas=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Queries\n",
    "\n",
    "Some helper functions for exploring the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verse_by_id(chapter: int, verse: int) -> str:\n",
    "    \"\"\"\n",
    "    Get a specific verse by chapter and verse number.\n",
    "    \"\"\"\n",
    "    verse_id = f\"{chapter}.{verse}\"\n",
    "    result = collection.get(ids=[verse_id])\n",
    "    \n",
    "    if result['ids']:\n",
    "        metadata = result['metadatas'][0]\n",
    "        output = f\"\"\"\n",
    "ğŸ“– Bhagavad Gita {verse_id}\n",
    "{'='*80}\n",
    "\n",
    "ğŸ•‰ï¸ Sanskrit:\n",
    "{metadata['sanskrit']}\n",
    "\n",
    "ğŸŒ Translation:\n",
    "{metadata['translation']}\n",
    "\n",
    "ğŸ’¡ Commentary:\n",
    "{metadata.get('commentary', 'N/A')}\n",
    "\"\"\"\n",
    "        return output\n",
    "    else:\n",
    "        return f\"âŒ Verse {verse_id} not found in database.\"\n",
    "\n",
    "\n",
    "# Example: Get the famous verse 2.47\n",
    "print(get_verse_by_id(2, 47))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_chapter(query: str, chapter: int, n_results: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Search within a specific chapter only.\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” Searching in Chapter {chapter} for: '{query}'\\n\")\n",
    "    results = search_slokas(query, n_results=n_results, chapter_filter=chapter)\n",
    "    return format_search_results(results, show_commentary=False)\n",
    "\n",
    "\n",
    "# Example: Search for verses about dharma in Chapter 2\n",
    "print(search_by_chapter(\"duty and dharma\", chapter=2, n_results=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Database Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics about the database\n",
    "total_verses = collection.count()\n",
    "\n",
    "print(\"ğŸ“Š Bhagavad Gita Vector Database Statistics\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total verses in database: {total_verses}\")\n",
    "print(f\"Expected total verses: {sum(CHAPTER_VERSES.values())}\")\n",
    "print(f\"\\nChapter-wise verse count:\")\n",
    "\n",
    "for chapter in range(1, 19):\n",
    "    count = len(collection.get(where={\"chapter\": chapter})['ids'])\n",
    "    expected = CHAPTER_VERSES[chapter]\n",
    "    status = \"âœ…\" if count == expected else \"âš ï¸\"\n",
    "    print(f\"  Chapter {chapter:2d}: {count:2d}/{expected:2d} verses {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "### Enhancements You Can Make:\n",
    "\n",
    "1. **Web Interface**: Build a Streamlit or Gradio interface for easier interaction\n",
    "2. **Multiple Languages**: Add support for Hindi, Sanskrit responses\n",
    "3. **Context Memory**: Implement conversation history for follow-up questions\n",
    "4. **Fine-tuning**: Fine-tune the embedding model on Gita-specific corpus\n",
    "5. **Audio Output**: Add text-to-speech for verse recitation\n",
    "6. **Daily Verse**: Create a feature to get a random verse daily\n",
    "7. **Verse Comparison**: Compare interpretations from different commentators\n",
    "8. **Mind Map**: Visualize connections between verses on similar topics\n",
    "\n",
    "### Configuration Tips:\n",
    "\n",
    "1. **API Keys**: Set environment variables for security:\n",
    "   ```bash\n",
    "   export GROQ_API_KEY=\"your_key_here\"\n",
    "   export OPENAI_API_KEY=\"your_key_here\"\n",
    "   ```\n",
    "\n",
    "2. **Local LLM**: Use Ollama for completely local operation:\n",
    "   ```bash\n",
    "   ollama pull llama3.2\n",
    "   ```\n",
    "\n",
    "3. **Tuning Search**: Adjust `n_slokas` parameter (3-7 works best)\n",
    "\n",
    "4. **Database Reset**: To reload data from scratch, delete the `bg_vector_db` folder\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [Bhagavad Gita API Docs](https://github.com/vedicscriptures/bhagavad-gita)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [Groq API](https://console.groq.com/)\n",
    "\n",
    "ğŸ•‰ï¸ **May this tool help bring the timeless wisdom of the Bhagavad Gita to those who seek it!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
